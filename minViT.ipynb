{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MinViT\n",
    "\n",
    "In this notebook, I attempt to explain the vision transformer (ViT) architecture, which has found its way into computer vision as a powerful alternative to Convolutional Neural Networks (CNNs).\n",
    "\n",
    "This implementation will focus on classifying the CIFAR-10 dataset, but is adaptable to many tasks, including semantic segmentation, instance segmentation, and image generation.\n",
    "\n",
    "We begin by downloading the CIFAR-10 dataset, and transforming the data to `torch.Tensor`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset CIFAR10\n",
       "     Number of datapoints: 50000\n",
       "     Root location: ./data/cifar-10\n",
       "     Split: Train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "            ),\n",
       " Dataset CIFAR10\n",
       "     Number of datapoints: 10000\n",
       "     Root location: ./data/cifar-10\n",
       "     Split: Test\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "            ))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import math\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_data = datasets.CIFAR10(root='./data/cifar-10', train=True, download=True, transform=transform)\n",
    "test_data = datasets.CIFAR10(root='./data/cifar-10', train=False, download=True, transform=transform)\n",
    "\n",
    "train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images are represented as 3 channel (RGB) 32x32 pixel images. The dataset can be indexed, with the first index being the image index, and the second index indexing either the image data or the target. The pixel values are represented as `torch.float32` values from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 32, 32, 3), 50000)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.data.shape, len(train_data.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 32, 32), 6)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][0].numpy().shape, train_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2314, 0.1686, 0.1961,  ..., 0.6196, 0.5961, 0.5804],\n",
       "         [0.0627, 0.0000, 0.0706,  ..., 0.4824, 0.4667, 0.4784],\n",
       "         [0.0980, 0.0627, 0.1922,  ..., 0.4627, 0.4706, 0.4275],\n",
       "         ...,\n",
       "         [0.8157, 0.7882, 0.7765,  ..., 0.6275, 0.2196, 0.2078],\n",
       "         [0.7059, 0.6784, 0.7294,  ..., 0.7216, 0.3804, 0.3255],\n",
       "         [0.6941, 0.6588, 0.7020,  ..., 0.8471, 0.5922, 0.4824]]),\n",
       " torch.float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][0][0], train_data[0][0].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are familiar with the transformer architecture, you likely know that transformers work with vectors to model different modalities. For a text-based modality, this means somehow tokenizing a string of text into characters or larger chunks, and training an embedding table to represent each token as a vector. As an example, the string \"This is a test.\" may tokenize as follows:\n",
    "\n",
    "<span style=\"background-color:rgba(107,64,216,.3);white-space:pre;\">This</span><span style=\"background-color:rgba(104,222,122,.4);white-space:pre;\"> is</span><span style=\"background-color:rgba(244,172,54,.4);white-space:pre;\"> a</span><span style=\"background-color:rgba(239,65,70,.4);white-space:pre;\"> test</span><span style=\"background-color:rgba(39,181,234,.4);white-space:pre;\">.</span>\n",
    "\n",
    "To adapt the transformer architecture for image tasks, we need to represent image data as a sequence a vectors, similar to how text is tokenized. In the original [ViT paper](https://arxiv.org/abs/2010.11929), the authors address this by dividing an image into many patches and flattening them into vectors. With CIFAR-10, an image $x \\in \\mathbb{R}^{H\\times W\\times C}$ is turned into several flattened 2D patches of the form $x_p \\in \\mathbb{R}^{N\\times (P^2\\cdot C)}$, where $(H,W)$ are the image dimensions (32x32), $C$ is the number of channels (3 for RGB), and $P$ is the patch size. The number of flattened 2D patches is then $N = \\frac{HW}{P^2}$. Finally, we project the flattened patches to latent vectors of size $D$, using the linear projection $\\mathbf{E} \\in \\mathbb{R}^{(P^2\\cdot C)\\times D}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "patch_size = 4\n",
    "for i in range(0, 32, patch_size):\n",
    "    for j in range(0, 32, patch_size):\n",
    "        patch = train_data[0][0][:, i:i+patch_size, j:j+patch_size]\n",
    "\n",
    "print(patch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A naive implementation of turning the images into patches may look like the code above. However, we can accelerate this process by using `torch.Tensor.unfold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 0.,  1.,  2.,  3.],\n",
       "           [ 4.,  5.,  6.,  7.],\n",
       "           [ 8.,  9., 10., 11.],\n",
       "           [12., 13., 14., 15.]],\n",
       " \n",
       "          [[16., 17., 18., 19.],\n",
       "           [20., 21., 22., 23.],\n",
       "           [24., 25., 26., 27.],\n",
       "           [28., 29., 30., 31.]],\n",
       " \n",
       "          [[32., 33., 34., 35.],\n",
       "           [36., 37., 38., 39.],\n",
       "           [40., 41., 42., 43.],\n",
       "           [44., 45., 46., 47.]]]]),\n",
       " tensor([[[[[[ 0.,  1.],\n",
       "             [ 4.,  5.]],\n",
       " \n",
       "            [[ 2.,  3.],\n",
       "             [ 6.,  7.]]],\n",
       " \n",
       " \n",
       "           [[[ 8.,  9.],\n",
       "             [12., 13.]],\n",
       " \n",
       "            [[10., 11.],\n",
       "             [14., 15.]]]],\n",
       " \n",
       " \n",
       " \n",
       "          [[[[16., 17.],\n",
       "             [20., 21.]],\n",
       " \n",
       "            [[18., 19.],\n",
       "             [22., 23.]]],\n",
       " \n",
       " \n",
       "           [[[24., 25.],\n",
       "             [28., 29.]],\n",
       " \n",
       "            [[26., 27.],\n",
       "             [30., 31.]]]],\n",
       " \n",
       " \n",
       " \n",
       "          [[[[32., 33.],\n",
       "             [36., 37.]],\n",
       " \n",
       "            [[34., 35.],\n",
       "             [38., 39.]]],\n",
       " \n",
       " \n",
       "           [[[40., 41.],\n",
       "             [44., 45.]],\n",
       " \n",
       "            [[42., 43.],\n",
       "             [46., 47.]]]]]]))"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.arange(0.,48).reshape(1, 3, 4, 4) # batch size, channels, width, height\n",
    "image, image.unfold(2, 2, 2).unfold(3, 2, 2) # first unfold width, then height into 2x2 patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then view this tensor of 2D patches as 1D vectors using `Tensor.reshape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  4.,  5.],\n",
       "        [ 2.,  3.,  6.,  7.],\n",
       "        [ 8.,  9., 12., 13.],\n",
       "        [10., 11., 14., 15.],\n",
       "        [16., 17., 20., 21.],\n",
       "        [18., 19., 22., 23.],\n",
       "        [24., 25., 28., 29.],\n",
       "        [26., 27., 30., 31.],\n",
       "        [32., 33., 36., 37.],\n",
       "        [34., 35., 38., 39.],\n",
       "        [40., 41., 44., 45.],\n",
       "        [42., 43., 46., 47.]])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.unfold(2,2,2).unfold(3,2,2).reshape(-1, 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another embedding that is critical for the transformer architecture to understand context is the positional embedding. In the text modality, the positional embedding is often implemented using cosine functions. To represent 2D position however, we will use a standard embedding table that is learned during training. There are other possibilities for representing position, including 2D-aware positional embeddings, but these are harder to implement and result in negligble performance differences.\n",
    "\n",
    "Finally, we also want to use a specific vector for information about the class of the image to the transformer input. Through each transformer block, this vector is modified until the end, when it is fed into a multi-layer perceptron to determine the class of the image. We will use a learnable embedding for this and prepend it to the other embedded patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size = 32, patch_size = 4, in_chans = 3, embed_dim = 768):\n",
    "        super().__init__()\n",
    "        self.img_size   = img_size\n",
    "        self.patch_size = patch_size    # P\n",
    "        self.in_chans   = in_chans      # C\n",
    "        self.embed_dim  = embed_dim     # D\n",
    "\n",
    "        self.num_patches = (img_size // patch_size) ** 2        # N = H*W/P^2\n",
    "        self.flatten_dim = patch_size * patch_size * in_chans   # P^2*C\n",
    "        \n",
    "        self.proj = nn.Linear(self.flatten_dim, embed_dim) # (P^2*C,D)\n",
    "\n",
    "        self.position_embed = nn.Parameter(torch.zeros(1, 1 + self.num_patches, embed_dim))\n",
    "        self.class_embed    = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "        x = x.contiguous().view(B, C, self.num_patches, -1)\n",
    "        x = x.permute(0, 2, 1, 3).reshape(B, self.num_patches, -1)\n",
    "\n",
    "        x = self.proj(x)\n",
    "\n",
    "        cls_emb = self.class_embed.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_emb, x), dim = 1)\n",
    "\n",
    "        x = x + self.position_embed\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.0569, -0.0865,  0.0411,  ...,  0.1318,  0.1935, -0.0940],\n",
       "          [ 0.0846,  0.1228, -0.0564,  ...,  0.0554,  0.4496,  0.0409],\n",
       "          ...,\n",
       "          [ 0.0518,  0.1557, -0.0584,  ...,  0.1239,  0.3988,  0.0172],\n",
       "          [ 0.0330,  0.0413, -0.0276,  ...,  0.1904,  0.3197,  0.0385],\n",
       "          [ 0.0961,  0.1149, -0.0723,  ...,  0.2575,  0.2643,  0.0029]],\n",
       " \n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0936,  0.1547,  0.0363,  ...,  0.2752,  0.4989, -0.0243],\n",
       "          [-0.0062,  0.1697,  0.2444,  ...,  0.2540,  0.8188, -0.0052],\n",
       "          ...,\n",
       "          [-0.0646, -0.0800,  0.0320,  ...,  0.2009,  0.2252, -0.0501],\n",
       "          [-0.0562, -0.0649,  0.1433,  ...,  0.2242,  0.4622, -0.0627],\n",
       "          [-0.0676, -0.0061,  0.0631,  ...,  0.1936,  0.4408, -0.0672]],\n",
       " \n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.2059,  0.3761,  0.2089,  ...,  0.3317,  1.0730,  0.1296],\n",
       "          [ 0.2036,  0.3759,  0.2096,  ...,  0.3301,  1.0744,  0.1280],\n",
       "          ...,\n",
       "          [-0.0487,  0.0336,  0.0072,  ...,  0.2073,  0.3094, -0.0495],\n",
       "          [-0.0475,  0.0099,  0.0080,  ...,  0.2000,  0.2630, -0.0526],\n",
       "          [-0.0308,  0.0246,  0.0320,  ...,  0.1848,  0.3298, -0.0503]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.1034, -0.0812,  0.0224,  ...,  0.1708,  0.1767, -0.0975],\n",
       "          [-0.0136, -0.0185, -0.0354,  ...,  0.2350,  0.1323, -0.0379],\n",
       "          ...,\n",
       "          [ 0.0946,  0.2360,  0.0255,  ...,  0.1865,  0.4423,  0.0478],\n",
       "          [ 0.0286,  0.2189,  0.0779,  ...,  0.1610,  0.4856,  0.0114],\n",
       "          [ 0.0376,  0.2499,  0.0249,  ...,  0.1911,  0.4682,  0.0318]],\n",
       " \n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0323,  0.1331,  0.2017,  ...,  0.4021,  0.6738,  0.0127],\n",
       "          [ 0.0433,  0.1248,  0.2113,  ...,  0.3951,  0.6863,  0.0355],\n",
       "          ...,\n",
       "          [ 0.1324,  0.2848,  0.1918,  ...,  0.3792,  0.8730,  0.1035],\n",
       "          [-0.0196,  0.0223, -0.0785,  ...,  0.4770,  0.0991, -0.0401],\n",
       "          [-0.1064, -0.0584,  0.1197,  ...,  0.3300,  0.2930, -0.0788]],\n",
       " \n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0875,  0.2250,  0.0105,  ...,  0.2142,  0.5908, -0.0091],\n",
       "          [ 0.0125,  0.2821,  0.0916,  ...,  0.2234,  0.8410,  0.0107],\n",
       "          ...,\n",
       "          [-0.0212,  0.1425, -0.0541,  ...,  0.0847,  0.4260, -0.0481],\n",
       "          [ 0.0091,  0.1690, -0.0300,  ...,  0.1097,  0.4691, -0.0197],\n",
       "          [ 0.0186,  0.1685, -0.0214,  ...,  0.1248,  0.4878, -0.0140]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " torch.Size([10, 65, 768]))"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_embed = PatchEmbedding()\n",
    "\n",
    "embeddings = patch_embed(torch.stack([train_data[i][0] for i in range(10)]))\n",
    "\n",
    "embeddings, embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, we are able to embed batches into our desired embedding dimension, with a correct number of vectors $N+1$.\n",
    "\n",
    "We can now continue to implement the standard transformer architecture, with one notable change from GPT-like architectures. The attention mechanism of GPT involves multi-headed **causal** self-attention, which means that vectors are only allowed to query and interact with previous vectors. Although this makes sense in a language model that wants to extract causal contextual information, we want all vectors to communicate with all other vectors, and want to prevent applying an attention mask. Otherwise, the implementation remains unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutliheadedSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim = 768, num_heads = 4, bias = False, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "\n",
    "        self.embed_dim   = embed_dim\n",
    "        self.num_heads   = num_heads\n",
    "        self.head_dim    = embed_dim // num_heads\n",
    "\n",
    "        self.query   = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.key     = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.value   = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "        self.out     = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, _ = x.size()\n",
    "\n",
    "        q = self.query(x).view(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        k = self.key(x).view(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        v = self.value(x).view(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        # do NOT use causal attention as we are not dealing with sequential data (image patches are unordered)\n",
    "        attn = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.head_dim))\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        \n",
    "        #y = torch.nn.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout=self.attn_dropout if self.training else 0, is_causal=False)\n",
    "        #y = y.transpose(1, 2).contiguous().view(B, N, C)\n",
    "        #y = self.resid_dropout(self.c_proj(y))\n",
    "\n",
    "        out = (attn @ v).permute(0, 2, 1, 3).contiguous().view(B, N, self.embed_dim)\n",
    "\n",
    "        out = self.out(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1757, -0.2464, -0.4134,  ..., -0.3564,  0.0455,  0.1032],\n",
       "         [-0.1715, -0.2462, -0.4092,  ..., -0.3610,  0.0515,  0.1003],\n",
       "         [-0.1724, -0.2474, -0.4128,  ..., -0.3645,  0.0555,  0.1002],\n",
       "         ...,\n",
       "         [-0.1710, -0.2473, -0.4098,  ..., -0.3666,  0.0564,  0.0996],\n",
       "         [-0.1696, -0.2475, -0.4091,  ..., -0.3672,  0.0566,  0.0992],\n",
       "         [-0.1683, -0.2473, -0.4068,  ..., -0.3709,  0.0577,  0.0993]],\n",
       "\n",
       "        [[-0.1521, -0.2388, -0.6402,  ..., -0.4138,  0.0234,  0.1322],\n",
       "         [-0.1376, -0.2526, -0.6332,  ..., -0.4497,  0.0539,  0.1275],\n",
       "         [-0.1403, -0.2566, -0.6437,  ..., -0.4529,  0.0592,  0.1265],\n",
       "         ...,\n",
       "         [-0.1418, -0.2423, -0.6297,  ..., -0.4284,  0.0398,  0.1270],\n",
       "         [-0.1387, -0.2435, -0.6282,  ..., -0.4340,  0.0466,  0.1253],\n",
       "         [-0.1456, -0.2463, -0.6343,  ..., -0.4366,  0.0401,  0.1279]],\n",
       "\n",
       "        [[-0.1408, -0.2385, -0.6398,  ..., -0.4515,  0.0103,  0.1567],\n",
       "         [-0.0718, -0.3082, -0.6767,  ..., -0.5506,  0.1018,  0.1481],\n",
       "         [-0.0720, -0.3079, -0.6766,  ..., -0.5502,  0.1016,  0.1480],\n",
       "         ...,\n",
       "         [-0.1147, -0.2503, -0.6260,  ..., -0.4757,  0.0344,  0.1487],\n",
       "         [-0.1156, -0.2488, -0.6250,  ..., -0.4735,  0.0335,  0.1478],\n",
       "         [-0.1118, -0.2518, -0.6274,  ..., -0.4774,  0.0385,  0.1466]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.1529, -0.2806, -0.5699,  ..., -0.3952,  0.0142,  0.1350],\n",
       "         [-0.1499, -0.2809, -0.5640,  ..., -0.4026,  0.0229,  0.1304],\n",
       "         [-0.1499, -0.2830, -0.5652,  ..., -0.4066,  0.0259,  0.1300],\n",
       "         ...,\n",
       "         [-0.1471, -0.2868, -0.5723,  ..., -0.4175,  0.0410,  0.1233],\n",
       "         [-0.1474, -0.2851, -0.5727,  ..., -0.4148,  0.0406,  0.1228],\n",
       "         [-0.1496, -0.2852, -0.5725,  ..., -0.4150,  0.0386,  0.1244]],\n",
       "\n",
       "        [[-0.0733, -0.2006, -0.8358,  ..., -0.4129, -0.0643,  0.1776],\n",
       "         [-0.0630, -0.2090, -0.8365,  ..., -0.4342, -0.0469,  0.1701],\n",
       "         [-0.0628, -0.2090, -0.8366,  ..., -0.4342, -0.0468,  0.1700],\n",
       "         ...,\n",
       "         [-0.0599, -0.2124, -0.8378,  ..., -0.4372, -0.0401,  0.1680],\n",
       "         [-0.0659, -0.2054, -0.8265,  ..., -0.4333, -0.0559,  0.1737],\n",
       "         [-0.0693, -0.2012, -0.8330,  ..., -0.4238, -0.0591,  0.1732]],\n",
       "\n",
       "        [[-0.1300, -0.0819, -0.3024,  ..., -0.2382, -0.0307,  0.0224],\n",
       "         [-0.1182, -0.0915, -0.3012,  ..., -0.2684, -0.0097,  0.0184],\n",
       "         [-0.1170, -0.0938, -0.3060,  ..., -0.2698, -0.0038,  0.0179],\n",
       "         ...,\n",
       "         [-0.1218, -0.0839, -0.2951,  ..., -0.2508, -0.0170,  0.0181],\n",
       "         [-0.1198, -0.0858, -0.2952,  ..., -0.2546, -0.0146,  0.0181],\n",
       "         [-0.1188, -0.0868, -0.2954,  ..., -0.2568, -0.0132,  0.0180]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSA = MutliheadedSelfAttention()\n",
    "LN = nn.LayerNorm(embeddings.shape, bias=False)\n",
    "\n",
    "MSA(LN(embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want to implement the multi-layer perceptron and combine all our modules into the transformer block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, embed_dim = 768, bias = False, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(embed_dim, embed_dim * 4, bias=bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(embed_dim * 4, embed_dim, bias=bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim = 768, bias = False):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(embed_dim, bias=bias)\n",
    "        self.attn = MutliheadedSelfAttention()\n",
    "        self.ln_2 = nn.LayerNorm(embed_dim, bias=bias)\n",
    "        self.mlp = MLP()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final element to consider in our model is the output. Unlike a transformer model like GPT, we would like to produce a probability distribution of the various image classes in CIFAR-10. To achieve this, we use the class vector prepended to the input that was mentioned earlier. After the vector has passed through each transformer block, we can finally take the class vector and pass it through a linear projection once more to get a probability distribution across all ten image classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim = 768, num_layers = 4, bias = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            pe = PatchEmbedding(),\n",
    "            drop = nn.Dropout(0.1),\n",
    "            h = nn.ModuleList([Block() for _ in range(num_layers)]),\n",
    "            ln_f = nn.LayerNorm(embed_dim)\n",
    "        ))\n",
    "        self.head = nn.Linear(embed_dim, 10, bias=False)\n",
    "\n",
    "\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        return n_params\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        emb = self.transformer.pe(x)\n",
    "        x = self.transformer.drop(emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        class_token = x[:, 0]\n",
    "        logits = self.head(class_token)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 28.42M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6155,  0.0833,  0.3612,  0.1262,  0.3042,  0.1746, -0.8091, -0.1743,\n",
       "          0.7834, -0.0631],\n",
       "        [-1.0184, -0.1414, -0.1417, -0.2963,  0.3737,  0.0205, -0.6493, -0.0109,\n",
       "          0.8699, -0.1459],\n",
       "        [-0.9966,  0.1542, -0.1182, -0.0219,  0.5765,  0.0459, -0.4171, -0.1138,\n",
       "          0.9797, -0.0116],\n",
       "        [-0.7114,  0.0771,  0.1259,  0.0709,  0.4262,  0.2157, -0.9031, -0.0468,\n",
       "          0.7561, -0.0685],\n",
       "        [-0.7684,  0.0140, -0.1220, -0.2881,  0.6543, -0.0601, -0.4210, -0.1563,\n",
       "          0.8449,  0.0266],\n",
       "        [-0.6628,  0.2121, -0.0296,  0.1316,  0.4504,  0.5865, -1.1234, -0.1563,\n",
       "          0.6179,  0.0671],\n",
       "        [-0.8797,  0.1741,  0.2067, -0.1092,  0.4820,  0.0855, -0.3633,  0.1939,\n",
       "          0.8685, -0.4827],\n",
       "        [-0.8408,  0.1524, -0.1209, -0.2138,  0.3352,  0.1860, -0.5956,  0.1675,\n",
       "          0.9737,  0.0405],\n",
       "        [-0.8931, -0.3047, -0.1372, -0.3863,  0.4625,  0.2419, -0.4026,  0.1199,\n",
       "          0.7785,  0.1355],\n",
       "        [-0.4177,  0.0746,  0.1027,  0.1930,  0.4062,  0.0012, -0.6935, -0.1216,\n",
       "          1.0843, -0.0408]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit = ViT()\n",
    "vit(torch.stack([train_data[i][0] for i in range(10)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
