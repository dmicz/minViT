{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MinViT\n",
    "\n",
    "In this notebook, I attempt to explain the vision transformer (ViT) architecture, which has found its way into computer vision as a powerful alternative to Convolutional Neural Networks (CNNs).\n",
    "\n",
    "This implementation will focus on classifying the CIFAR-10 dataset, but is adaptable to many tasks, including semantic segmentation, instance segmentation, and image generation.\n",
    "\n",
    "We begin by downloading the CIFAR-10 dataset, and transforming the data to `torch.Tensor`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset CIFAR10\n",
       "     Number of datapoints: 50000\n",
       "     Root location: ./data/cifar-10\n",
       "     Split: Train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "            ),\n",
       " Dataset CIFAR10\n",
       "     Number of datapoints: 10000\n",
       "     Root location: ./data/cifar-10\n",
       "     Split: Test\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "            ))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_data = datasets.CIFAR10(root='./data/cifar-10', train=True, download=True, transform=transform)\n",
    "test_data = datasets.CIFAR10(root='./data/cifar-10', train=False, download=True, transform=transform)\n",
    "\n",
    "train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images are represented as 3 channel (RGB) 32x32 pixel images. The dataset can be indexed, with the first index being the image index, and the second index indexing either the image data or the target. The pixel values are represented as `torch.float32` values from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 32, 32, 3), 50000)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.data.shape, len(train_data.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 32, 32), 6)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][0].numpy().shape, train_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2314, 0.1686, 0.1961,  ..., 0.6196, 0.5961, 0.5804],\n",
       "         [0.0627, 0.0000, 0.0706,  ..., 0.4824, 0.4667, 0.4784],\n",
       "         [0.0980, 0.0627, 0.1922,  ..., 0.4627, 0.4706, 0.4275],\n",
       "         ...,\n",
       "         [0.8157, 0.7882, 0.7765,  ..., 0.6275, 0.2196, 0.2078],\n",
       "         [0.7059, 0.6784, 0.7294,  ..., 0.7216, 0.3804, 0.3255],\n",
       "         [0.6941, 0.6588, 0.7020,  ..., 0.8471, 0.5922, 0.4824]]),\n",
       " torch.float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][0][0], train_data[0][0].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are familiar with the transformer architecture, you likely know that transformers deal with \n",
    "\n",
    "<span style=\"background-color:rgba(107,64,216,.3);white-space:pre;\">This</span><span style=\"background-color:rgba(104,222,122,.4);white-space:pre;\"> is</span><span style=\"background-color:rgba(244,172,54,.4);white-space:pre;\"> a</span><span style=\"background-color:rgba(239,65,70,.4);white-space:pre;\"> test</span><span style=\"background-color:rgba(39,181,234,.4);white-space:pre;\">.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "patch_size = 4\n",
    "for i in range(0, 32, patch_size):\n",
    "    for j in range(0, 32, patch_size):\n",
    "        patch = train_data[0][0][:, i:i+patch_size, j:j+patch_size]\n",
    "        print(patch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
